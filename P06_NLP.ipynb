{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# P06 - Avis Restau - Détecter les \"bad buzz\" laissés dans les commentaires concernant un restaurant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "WuERQV37fwyf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IiGoWiBSbuA0"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import tarfile\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLP\n",
    "import re\n",
    "import nltk\n",
    "#nltk.download()\n",
    "import spacy\n",
    "import gensim\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Analyse et traitements des fichiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "path = ~/file\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my_tar = tarfile.open('yelp_dataset.tar'\n",
    "#my_tar.extractall()\n",
    "#my_tar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Après cette étpade de décompression, nous nous retrouvons avec 5 fichiers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "BhgvsJZF66zN"
   },
   "outputs": [],
   "source": [
    "file_business = 'yelp_academic_dataset_business.json'\n",
    "file_checking = 'yelp_academic_dataset_checkin.json'\n",
    "file_review = 'yelp_academic_dataset_review.json'\n",
    "file_tip = 'yelp_academic_dataset_tip.json'\n",
    "file_user = 'yelp_academic_dataset_user.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Je commence par regarder la nature de chaque fichier (les différentes variables) pour voir quelles seront les informations à conserver dans le cadre de mon travail, à savoir de détecter les \"bad buzz\" concernant chaque restaurant.  \n",
    "Cela passera par deux aspects:  \n",
    "    - identifier les établissements qui sont bel et bien des restaurants.  \n",
    "    - trier les bons et mauvais avis en fonction de chaque note. Cette dernière allant de 1 à 5, je me limiterai aux avis ayant une note égale à 1 ou 2  \n",
    "Une fois cela fait, alors je pourrai me pencher plus en détail sur les commentiares en question"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Déjà, j'en regarde la longueur. En effet, sachant que l'ensemble des 5 fichiers prend plus de 10 Go en mémoire, il est plus que probable qu'il faudra ne garder qu'une portion de chaque fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_line_file(file):\n",
    "    line_count = 0\n",
    "    with open(file, 'r') as f:\n",
    "        while f.readline():\n",
    "            line_count+=1\n",
    "    return line_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le fichier business possède 160585 lignes\n",
      "le fichier checking possède 138876 lignes\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 89] Operation canceled",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ym/0kvnkj911t7crk_w7n6qst6h0000gn/T/ipykernel_21039/1895372757.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'le fichier business possède'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount_line_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_business\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lignes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'le fichier checking possède'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount_line_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_checking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lignes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'le fichier review possède'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount_line_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_review\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lignes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'le fichier tip possède'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount_line_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_tip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lignes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'le fichier user possède'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcount_line_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_user\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lignes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/ym/0kvnkj911t7crk_w7n6qst6h0000gn/T/ipykernel_21039/2561040341.py\u001b[0m in \u001b[0;36mcount_line_file\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mline_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m             \u001b[0mline_count\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mline_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 89] Operation canceled"
     ]
    }
   ],
   "source": [
    "print('le fichier business possède',count_line_file(file_business), 'lignes')\n",
    "print('le fichier checking possède',count_line_file(file_checking), 'lignes')\n",
    "print('le fichier review possède',count_line_file(file_review), 'lignes')\n",
    "print('le fichier tip possède',count_line_file(file_tip), 'lignes')\n",
    "print('le fichier user possède',count_line_file(file_user), 'lignes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit qu'il y a 3 fichiers très volumineux (review, tip et user) qui, chacun, possède plus d'1 million de lignes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, pour avoir une idée plus précise des informations (variables) de chaque fichier j'en regarde la première ligne dans un dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Je définis une fonction qui me permettra d'obtenir une liste contenant les n premières lignes d'un fichier\n",
    "\n",
    "def file_to_df_first_n_lines(file, n):\n",
    "    liste = []\n",
    "    df = pd.DataFrame()\n",
    "    with open(file, 'r') as f:\n",
    "        for i in range(n):\n",
    "                line = f.readline()\n",
    "                dico = json.loads(line)\n",
    "                df = df.append(dico, ignore_index=True )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "J'affiche la 1ère ligne de chaque dataframe obtenu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_to_df_first_n_lines(file_business, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, éventuellement le business_id, le nom, le nombre d'étoiles (stars) et la catégorie seront à garder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_df_first_n_lines(file_checking, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... Pour ce fichier, aucune information utile!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_df_first_n_lines(file_review, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, pour pouvoir ne garder que les avis des restaurants, il faudra garder:  \n",
    "    - le business_id      \n",
    "    - le user_id    \n",
    "    - \"stars\"   \n",
    "    - et évidemment, \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_df_first_n_lines(file_tip, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"text\" (en tant que \"tip\") pourrait aussi être important pour nous (éventuellement) donc faire le lien avec le resrte je garderai \"user_id\", \"business_id\" et \"text\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_to_df_first_n_lines(file_user, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Là il ne s'agit que de la fiche d'information d'un utilisateur, qui ne sera pas nécessaire pour la suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A présent, je vais construire les dataframes selon les informations qui nous serviront part la suite:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Je définis la fonction qui, à partir d'un fichier:\n",
    "### ne gardera que n lignes\n",
    "### et se conterera, pour chaque ligne JSON du fichier, de n'en garder que les clés importantes (columns)\n",
    "### pour finalement transfomer ce résultat en un dataframe\n",
    "\n",
    "def file_to_df(file, n, columns):\n",
    "    df = pd.DataFrame()\n",
    "    with open(file, 'r') as f1:\n",
    "        for i in range(n):\n",
    "            line = f1.readline()\n",
    "            dico = json.loads(line)\n",
    "            dico = {c:dico[c] for c in columns}\n",
    "            df = df.append(dico, ignore_index = True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#df_business = file_to_df(file_business, count_line_file(file_business), ['business_id', 'stars', 'categories'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**14 minutes** pour traiter **160 000 lignes**... Pour les 2 fichiers qu'il me reste (file_review et file_tip), je me contenterai donc de ne garder que **150 000** lignes chacun... et pour éviter de relancer la cellule ultérieurement, j'en fais une sauvegarde avec le module \"pickle\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# je sauvegarde le dataframe précédent\n",
    "\n",
    "file = open('df_business_saved', 'wb')\n",
    "pickle.dump(df_business, file)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... que je recharge:\n",
    "file = open('df_business_saved', 'rb')\n",
    "df_business = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#df_review = file_to_df(file_review, 150000, ['user_id', 'business_id', 'stars', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# je sauvegarde le dataframe précédent\n",
    "\n",
    "file = open('df_review_saved', 'wb')\n",
    "pickle.dump(df_review, file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... que je recharge:\n",
    "file = open('df_review_saved', 'rb')\n",
    "df_review = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#df_tip = file_to_df(file_tip, 150000, ['user_id','business_id', 'text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_tip.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business  = df_business[df_business['categories'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurant = df_business[df_business['categories'].str.contains('Restaurants')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_business['categories'].str.contains('Restaurants').isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurant.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants_reviews = df_review[df_review['business_id'].isin(df_restaurant['business_id'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants_reviews.shape, df_review.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u_WvLXPi79ba"
   },
   "outputs": [],
   "source": [
    "df_restaurants_reviews = df_restaurants_reviews[df_restaurants_reviews['stars']<3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_restaurants_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_restaurants = df_restaurants_reviews['business_id'].nunique()\n",
    "print('il y a ', nb_restaurants, 'restaurants différents parmi les 20440 mauvais commentaires (note<3) laissés par les clients')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Je sauvegarde ces résultats (le dataframe obtenu), dans un fichier: peut-être nous sera-t-il utile pour la suite...\n",
    "df_restaurants_reviews.to_csv('restaurants_bad_reviews.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Traitement des données textuelles - NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que nous avons gardé:  \n",
    "    - les restaurants parmi les différents établissements.  \n",
    "    - et au sein de ceux-ci, seuls les restaurants ayant reçu des mauvais avis (c'est à dire des notes <3)  \n",
    "Alors on peut s'attaquer à la partie \"traitement du langage\", appelé en anglais NLP (Natural langage processing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Normalisation des commentaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une des premières étapes pour ce faire est la NORMALISATION des données: il s'agit de convertir chaque mot dans une forme **canonique** en leur appliquant certaines opérations:\n",
    "- la **tokenisation** (à savoir \"éclater\" le texte en différents mots)\n",
    "- enlever ce qu'on appelle les **\"stop words\"**, mots qui sont bien sûr utiles dans la compréhension du discours humain, mais pas pour le NLP \n",
    "- enfin, le **POS-tagging (POS pour part of speech)** et la **lemmatisation**, qui consistent  à ramener un mot à sa racine grammaticale (le vocable)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.1 Nettoyage des données: tokenisation, enlèvement des \"stop words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv('restaurants_bad_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_reviews = reviews.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_reviews = reviews['text'].values.tolist()\n",
    "# list_of_reviews est une liste contenant nb_reviews (donc 20440) éléments qui sont eux des chaînes de caractères (str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la tokenisation et la suppression des \"stop words\"se feront grâce aux méthodes de la librairie [NLTK]('https://www.nltk.org/')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple:\n",
    "comment = reviews.iloc[2]['text']\n",
    "print(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def normalisation(text):\n",
    "    #transforme le texte en tokens\n",
    "    tokens = word_tokenize(text)\n",
    "    #convertit les majuscules en minusucles\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    stripped = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    words = [word for word in stripped if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    return words\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(normalisation(comment))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "la fonction \"normalisation\" a donc transformé le commentaire en une liste ne contenant plus que les \"tokens\", sans les \"stop words\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A présent, nous pouvons appliquer cette fonction sur l'ensemble des commentaires de notre \"list_of_reviews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_reviews_normalized = [normalisation(elt) for elt in list_of_reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list_of_reviews_normalized[2], end= ' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 POS tagging et lemmatisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "le **POS (part of speech) tagging** consiste à identifier les mots selon leur fonction dans la phrase (nom, adjectif, verbe...)\n",
    "Quant à la **lemmatisation**, elle ramène les mots à leur forme canonique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour Les deux, j'utiliserai les méthodes embarquées par la librairie \"[**SpaCy**](https://spacy./)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque:** ainsi que mentionné ici [Machine learning mastery](https://machinelearningmastery.com/gentle-introduction-bag-words-model/ ),  \n",
    "\"A more sophisticated approach is to create a vocabulary of grouped words. This both changes the scope of the vocabulary and allows the bag-of-words to capture a little bit more meaning from the document.\"  \n",
    "\n",
    "Donc, pour pouvoir avoir une meilleure analyse des mauvais commentaires, il apparait judicieux de prévoir, pour notre construction de **\"bag of words\"** future, de considérer ce qu'on appelle un bigramme (2-gram en anglais) qui nous permettra d'isoler une expression telle que \"mauvais service\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm #import du modèle \"en_core_web_sm\" du vocabulaire anglais, de sa syntaxe et de sesn entités\n",
    "nlp = en_core_web_sm.load() #chargement de ce modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_to_string (liste):\n",
    "    return (' '.join(liste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# je crée la liste des TAGs pour notre futur bigram\n",
    "\n",
    "TAG_list = ['NOUN'] #on ne garde que les noms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''je définis une fonction qui prend en entrée un texte et retourne les NOMS COMMUNS contenus dans ce texte\n",
    " la fonction:\n",
    "     -> prend en entrée une chaïne de caractères\n",
    "     -> renvoie une chaîne de caractères'''\n",
    "\n",
    "def POS_tagging (text):\n",
    "    tag = nlp(text)\n",
    "    l = [word.text for word in tag if word.pos_ in TAG_list]\n",
    "    return ' '.join(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_tagging(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''je définis une fonction qui prend en entrée un texte et retourne les mots du texte sous forme canonique (leur \"lemme\")\n",
    " la fonction:\n",
    "     -> prend en entrée une chaïne de caractères\n",
    "     -> renvoie une chaîne de caractères'''\n",
    "\n",
    "def lemmatisation (text):\n",
    "    lem = nlp(text)\n",
    "    l = [word.lemma_ for word in lem]\n",
    "    return ' '.join(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A présent on peu enchaîner les deux fonctions: testons les sur le commentaire défini précédemment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('voici le commentaire en question:\\n\\n', comment)\n",
    "text = POS_tagging(comment)\n",
    "text = lemmatisation(text)\n",
    "print('\\n\\nvoici le commentaire dont il ne reste que les noms communs ayant été par la suite lemmatisés\\n', text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "il ne nous reste plus qu'à transformer tous nos commentaires de notre \"list_of_reviews_normalized\".  \n",
    "**Rq** : Cette \"list_of_reviews_normalized\" étant justement une liste quand il nous faut une chaîne de caractères pour le POS_Tagging, alors il me faudra effectuer cette première étape\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cleaned_list = []\n",
    "for i in range(nb_reviews):\n",
    "    liste = list_of_reviews_normalized[i]\n",
    "    text = ' '.join(liste)\n",
    "    text = POS_tagging(text)\n",
    "    text = lemmatisation(text)\n",
    "    cleaned_list.append(text)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['cleaned_text'] = pd.DataFrame(cleaned_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque importante!!!** En ne gardant que les noms, on voit que l'on perd beaucoup d'informations. Je vais donc refaire la procédure en incluant cette fois les adjectifs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "TAG_list = ['NOUN', 'ADJ'] # cette fois j'inclue les adjectifs\n",
    "\n",
    "cleaned_list = []\n",
    "for i in range(nb_reviews):\n",
    "    liste = list_of_reviews_normalized[i]\n",
    "    text = ' '.join(liste)\n",
    "    text = POS_tagging(text)\n",
    "    text = lemmatisation(text)\n",
    "    cleaned_list.append(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews['cleaned_text'] = pd.DataFrame(cleaned_list)\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Création du \"bag of words\" avec Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout ce qui suit est inspiré de cette [page](https://getdoc.wiki/Gensim-creating-a-bag-of-words-corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_list_tokenized = [word_tokenize(elt) for elt in reviews['cleaned_text']]\n",
    "print(cleaned_list_tokenized[2], end = ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import gensim.corpora as corpora\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "#Je crée un objet \"dictionnary\"\n",
    "id2word = corpora.Dictionary(cleaned_list_tokenized)\n",
    "\n",
    "# Je crée à présent un corpus de \"bag of words\" ainsi:\n",
    "corpus = [id2word.doc2bow(doc, allow_update=True) for doc in cleaned_list_tokenized]\n",
    "#print(BOW_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[0])\n",
    "print(len(corpus[0]))\n",
    "print(len(cleaned_list_tokenized[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici, on voit que notre 1ère ligne:  \n",
    "\"many marriott huge disappointment front desk atrium nice starbuck site nice room old flat screen hotel priceline rate good deal price true renaissance\"  \n",
    "a été transformée en une liste de couples d'éléments où:  \n",
    "le 1er terme est la position du mot dans la phrase. \n",
    "le 2nd est son nombre d'occurences. \n",
    "\n",
    "ex: (3,1) représente le mot \"dissapointment\" et n'apparaît qu'une fois.  \n",
    "\n",
    "Remarque: nous avons (11,2) ce qui \"déplace d'un cran\" les autres termes vers la gauche et c'est pourquoi dans corpus[0] nous avons 22 éléments quand nous en avions 23 dans le texte d'origine\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le problème de notre \"BoW_corpus\" est qu'il n'est pas lisible en l'état mais avec une ligne de code en plus, on peut remédier à cela:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "id_words = [[(dictionary[id], count) for id, count in line] for line in BoW_corpus]\n",
    "print(id_words[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous avons notre **corpus** que nous pouvons enregistrer pour la suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.save('dictionary')\n",
    "corpora.MmCorpus.serialize('corpus', BoW_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word = dictionary.id2token\n",
    "temp = dictionary[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = corpora.MmCorpus('corpus')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Training parameters\n",
    "num_topics = 10\n",
    "chunksize = len(id2word)\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour la suite, voir [ici](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', model.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "coherence_model_lda = CoherenceModel(model=model, texts=cleaned_list_tokenized, dictionary=id2word, coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(model, corpus, dictionary, sort_topics = False)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(3, 22, 3):\n",
    "        model=LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=cleaned_list_tokenized, dictionary=id2word, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=cleaned_list_tokenized, start=3, limit=22, step=3)\n",
    "# Show graph\n",
    "import matplotlib.pyplot as plt\n",
    "limit=22; start=3; step=3;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On voit que le meilleur nombre de topics est de 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Training parameters\n",
    "num_topics = 15\n",
    "chunksize = len(id2word)\n",
    "passes = 20\n",
    "iterations = 400\n",
    "eval_every = None  # Don't evaluate model perplexity, takes too much time\n",
    "\n",
    "model = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(model, corpus, dictionary, sort_topics = False)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NPL1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
